name: mpt-7b-finance-hf-convert-final

compute:
  gpus: 8  # Number of GPUs to use

  ## These configurations are optional
  cluster: REPLACE_WITH_YOUR_CLUSTER # Name of the cluster to use for this run
  # gpu_type: a100_80gb # Type of GPU to use. We use a100_80gb in our experiments

integrations:
- integration_type: git_repo
  git_repo: mosaicml/llm-foundry
  # git_branch:  # use your branch
  # git_commit: # OR use your commit hash
  pip_install: -e .
  ssh_clone: false # Should be true if using a private repo

command: |
  cd llm-foundry/scripts/inference
  python convert_composer_to_hf.py \
    --composer_path REPLACE_WITH_YOUR_OBJECT_STORE://REPLACE_WITH_YOUR_BUCKET_NAME/sec_10k_demo/checkpoints/REPLACE_WITH_PREVIOUS_RUN_NAME/latest-rank0.pt.symlink \
    --hf_output_path REPLACE_WITH_YOUR_OBJECT_STORE://REPLACE_WITH_YOUR_BUCKET_NAME/sec_10k_demo/checkpoints/mpt-7b-hf/ \
    --output_precision bf16 \

image: mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04 # Use the Docker image provided by MosaicML
